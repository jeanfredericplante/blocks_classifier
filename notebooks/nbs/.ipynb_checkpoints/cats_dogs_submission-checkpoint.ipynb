{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "#import modules\n",
    "from utils import *\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "\n",
    "import os, json\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from enum import Enum\n",
    "from keras.layers.core import Dense\n",
    "from utils import save_array, load_array\n",
    "\n",
    "\n",
    "# DATA_HOME_DIR = '/srv/data/dogscats'\n",
    "DATA_HOME_DIR = '/home/ubuntu/blocks_classifier/data/dogscats' # on ec2\n",
    "\n",
    "reload(K)\n",
    "K.image_dim_ordering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/blocks_classifier/data/dogscats\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "\n",
    "#Set path to sample/ path if desired\n",
    "path = DATA_HOME_DIR + '/' #'/sample/'\n",
    "test_path = DATA_HOME_DIR + '/test/' #We use all the test data\n",
    "results_path=DATA_HOME_DIR + '/results/'\n",
    "train_path=path + '/train/'\n",
    "valid_path=path + '/valid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set constants. You can experiment with no_of_epochs to improve the model\n",
    "batch_size=128 # max i can go with the K80\n",
    "no_of_epochs=1 # seems to be optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(dirname, gen=image.ImageDataGenerator(), shuffle=True, batch_size=4, class_mode='categorical',\n",
    "                target_size=(224,224)):\n",
    "    return gen.flow_from_directory(dirname, target_size=target_size,\n",
    "                                   class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\n",
    "#     return gen.flow(dirname, target_size=target_size,\n",
    "#             class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "#Helper function to plot images by index in the validation set \n",
    "#Plots is a helper function in utils.py\n",
    "def plots_array(fpath, filenames, titles=None):\n",
    "    plots([image.load_img(fpath+f) for f in filenames], titles=titles)\n",
    "    \n",
    "\n",
    "    \n",
    "#Number of images to view for each visualization task\n",
    "n_view = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_classes = [\"cats\", \"dogs\"]\n",
    "\n",
    "base_model = VGG16(include_top=True, weights='imagenet') ## functional API\n",
    "## replaces last dense layer with another dense layer of size the number of classes\n",
    "x = base_model.layers[-2].output ## output is prior to last output\n",
    "predictions = Dense(len(output_classes), activation='softmax', name = \"predictions\")(x)\n",
    "ft_model = Model(input=base_model.input, output=predictions)\n",
    "optimizer = Adam(lr=0.001) #SGD(lr=0.001) Adam seems to work much better than SGD?\n",
    "\n",
    "## Freezes all but last layers\n",
    "for layer in ft_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "## Make the last 2 dense layers trainable\n",
    "for layer in ft_model.layers[-1:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "ft_model.compile(optimizer=optimizer,\n",
    "                loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "flow() got an unexpected keyword argument 'target_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-276e78108614>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-4ce9d4599b52>\u001b[0m in \u001b[0;36mget_batches\u001b[0;34m(dirname, gen, shuffle, batch_size, class_mode, target_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#             class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     return gen.flow(dirname, target_size=target_size,\n\u001b[0;32m----> 6\u001b[0;31m             class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: flow() got an unexpected keyword argument 'target_size'"
     ]
    }
   ],
   "source": [
    "batches = get_batches(train_path, batch_size=batch_size)\n",
    "val_batches = get_batches(valid_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "22872/23000 [============================>.] - ETA: 1s - loss: 0.0718 - acc: 0.9732"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "output of generator should be a tuple (x, y, sample_weight) or (x, y). Found: [[[[ 105.  104.  112.]\n   [ 101.  100.  108.]\n   [ 101.  100.  108.]\n   ..., \n   [ 104.   92.  102.]\n   [ 104.   92.  102.]\n   [  99.   87.   97.]]\n\n  [[  95.   94.  102.]\n   [  97.   96.  104.]\n   [  97.   96.  104.]\n   ..., \n   [ 103.   91.  101.]\n   [ 103.   91.  101.]\n   [  98.   86.   96.]]\n\n  [[  95.   94.  102.]\n   [  97.   96.  104.]\n   [  97.   96.  104.]\n   ..., \n   [ 103.   91.  101.]\n   [ 103.   91.  101.]\n   [  98.   86.   96.]]\n\n  ..., \n  [[  86.   76.   87.]\n   [  70.   60.   71.]\n   [  70.   60.   71.]\n   ..., \n   [  19.    9.   20.]\n   [  19.    9.   20.]\n   [  19.    9.   20.]]\n\n  [[  86.   76.   87.]\n   [  70.   60.   71.]\n   [  70.   60.   71.]\n   ..., \n   [  19.    9.   20.]\n   [  19.    9.   20.]\n   [  19.    9.   20.]]\n\n  [[  80.   70.   81.]\n   [  72.   62.   73.]\n   [  72.   62.   73.]\n   ..., \n   [  19.    9.   20.]\n   [  19.    9.   20.]\n   [  20.   10.   21.]]]\n\n\n [[[  85.   77.   75.]\n   [  88.   80.   78.]\n   [  87.   79.   77.]\n   ..., \n   [  78.   76.   64.]\n   [  78.   76.   64.]\n   [  80.   78.   66.]]\n\n  [[  88.   80.   78.]\n   [  88.   80.   78.]\n   [  86.   78.   76.]\n   ..., \n   [  79.   75.   64.]\n   [  79.   75.   64.]\n   [  79.   75.   64.]]\n\n  [[  84.   76.   74.]\n   [  91.   83.   81.]\n   [  91.   83.   81.]\n   ..., \n   [  85.   76.   67.]\n   [  84.   75.   66.]\n   [  84.   75.   66.]]\n\n  ..., \n  [[  58.   53.   50.]\n   [  57.   52.   49.]\n   [  57.   52.   49.]\n   ..., \n   [   9.    9.    9.]\n   [  10.   10.   10.]\n   [  13.   13.   13.]]\n\n  [[  61.   56.   53.]\n   [  58.   53.   50.]\n   [  57.   52.   49.]\n   ..., \n   [  10.   10.   10.]\n   [  10.   10.   10.]\n   [   9.    9.    9.]]\n\n  [[  57.   52.   48.]\n   [  56.   51.   47.]\n   [  58.   53.   49.]\n   ..., \n   [  18.   18.   18.]\n   [  15.   15.   15.]\n   [  12.   12.   12.]]]\n\n\n [[[ 134.  155.  174.]\n   [ 131.  152.  171.]\n   [ 129.  150.  169.]\n   ..., \n   [ 103.  122.  136.]\n   [ 103.  122.  136.]\n   [ 101.  120.  134.]]\n\n  [[ 130.  151.  170.]\n   [ 130.  151.  170.]\n   [ 130.  151.  170.]\n   ..., \n   [ 103.  122.  136.]\n   [ 103.  122.  136.]\n   [ 101.  120.  134.]]\n\n  [[ 127.  148.  167.]\n   [ 129.  150.  169.]\n   [ 130.  151.  170.]\n   ..., \n   [ 103.  122.  136.]\n   [ 103.  122.  136.]\n   [ 101.  120.  134.]]\n\n  ..., \n  [[ 152.  152.  126.]\n   [ 166.  164.  139.]\n   [ 171.  169.  144.]\n   ..., \n   [  17.   10.   18.]\n   [  17.   10.   18.]\n   [  17.   10.   18.]]\n\n  [[ 152.  152.  126.]\n   [ 164.  164.  138.]\n   [ 170.  168.  143.]\n   ..., \n   [  17.   10.   18.]\n   [  17.   10.   18.]\n   [  17.   10.   18.]]\n\n  [[ 151.  151.  125.]\n   [ 163.  163.  137.]\n   [ 169.  167.  142.]\n   ..., \n   [  17.   10.   18.]\n   [  17.   10.   18.]\n   [  17.   10.   18.]]]\n\n\n ..., \n [[[  15.   15.   15.]\n   [  15.   15.   15.]\n   [  38.   38.   38.]\n   ..., \n   [  18.   18.   18.]\n   [  18.   18.   18.]\n   [  18.   18.   18.]]\n\n  [[  15.   15.   15.]\n   [  15.   15.   15.]\n   [  38.   38.   38.]\n   ..., \n   [  18.   18.   18.]\n   [  18.   18.   18.]\n   [  18.   18.   18.]]\n\n  [[  15.   15.   15.]\n   [  15.   15.   15.]\n   [  35.   35.   35.]\n   ..., \n   [  18.   18.   18.]\n   [  18.   18.   18.]\n   [  18.   18.   18.]]\n\n  ..., \n  [[  79.   76.   95.]\n   [  79.   76.   95.]\n   [  79.   76.   95.]\n   ..., \n   [  61.   59.   72.]\n   [  59.   57.   70.]\n   [  59.   57.   70.]]\n\n  [[  77.   74.   93.]\n   [  77.   74.   93.]\n   [  77.   74.   93.]\n   ..., \n   [  60.   58.   71.]\n   [  59.   57.   70.]\n   [  59.   57.   70.]]\n\n  [[  77.   74.   93.]\n   [  77.   74.   93.]\n   [  77.   74.   93.]\n   ..., \n   [  60.   58.   71.]\n   [  59.   57.   70.]\n   [  59.   57.   70.]]]\n\n\n [[[ 154.  126.  122.]\n   [ 154.  126.  122.]\n   [ 158.  130.  126.]\n   ..., \n   [  94.   73.   72.]\n   [  99.   75.   75.]\n   [  99.   75.   75.]]\n\n  [[ 154.  126.  122.]\n   [ 154.  126.  122.]\n   [ 158.  130.  126.]\n   ..., \n   [  94.   73.   72.]\n   [  99.   75.   75.]\n   [  99.   75.   75.]]\n\n  [[ 158.  130.  126.]\n   [ 158.  130.  126.]\n   [ 159.  131.  127.]\n   ..., \n   [  85.   64.   63.]\n   [  85.   61.   61.]\n   [  85.   61.   61.]]\n\n  ..., \n  [[ 233.   87.  100.]\n   [ 233.   87.  100.]\n   [ 247.  101.  114.]\n   ..., \n   [ 199.   49.   61.]\n   [ 199.   49.   61.]\n   [ 199.   49.   61.]]\n\n  [[ 236.   90.  103.]\n   [ 236.   90.  103.]\n   [ 244.   98.  111.]\n   ..., \n   [ 198.   48.   60.]\n   [ 197.   46.   61.]\n   [ 197.   46.   61.]]\n\n  [[ 236.   90.  103.]\n   [ 236.   90.  103.]\n   [ 244.   98.  111.]\n   ..., \n   [ 198.   48.   60.]\n   [ 197.   46.   61.]\n   [ 197.   46.   61.]]]\n\n\n [[[  57.   59.   37.]\n   [  57.   59.   37.]\n   [  55.   57.   36.]\n   ..., \n   [  46.   24.    0.]\n   [  49.   33.    0.]\n   [  59.   47.    7.]]\n\n  [[  57.   59.   37.]\n   [  49.   51.   30.]\n   [  45.   47.   26.]\n   ..., \n   [  48.   26.    3.]\n   [  51.   35.    2.]\n   [  61.   49.    9.]]\n\n  [[  47.   49.   28.]\n   [  45.   47.   26.]\n   [  46.   48.   27.]\n   ..., \n   [  48.   26.    3.]\n   [  52.   35.    5.]\n   [  63.   51.   13.]]\n\n  ..., \n  [[  41.   36.   33.]\n   [  42.   37.   34.]\n   [  39.   34.   30.]\n   ..., \n   [  22.   17.   21.]\n   [  27.   22.   26.]\n   [  30.   25.   29.]]\n\n  [[  39.   33.   33.]\n   [  41.   36.   33.]\n   [  39.   34.   30.]\n   ..., \n   [  23.   18.   22.]\n   [  28.   23.   27.]\n   [  30.   25.   29.]]\n\n  [[  40.   34.   36.]\n   [  42.   36.   36.]\n   [  39.   34.   31.]\n   ..., \n   [  30.   25.   29.]\n   [  29.   24.   28.]\n   [  26.   21.   25.]]]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-209398bf271d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m ft_model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=no_of_epochs,\n\u001b[0;32m----> 2\u001b[0;31m                 validation_data=val_batches, nb_val_samples=val_batches.nb_sample)\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1583\u001b[0m                                 \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m                                 \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1585\u001b[0;31m                                 pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m   1586\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m                             \u001b[0;31m# no need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, val_samples, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1673\u001b[0m                     raise ValueError('output of generator should be a tuple '\n\u001b[1;32m   1674\u001b[0m                                      \u001b[0;34m'(x, y, sample_weight) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1675\u001b[0;31m                                      'or (x, y). Found: ' + str(generator_output))\n\u001b[0m\u001b[1;32m   1676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: output of generator should be a tuple (x, y, sample_weight) or (x, y). Found: [[[[ 105.  104.  112.]\n   [ 101.  100.  108.]\n   [ 101.  100.  108.]\n   ..., \n   [ 104.   92.  102.]\n   [ 104.   92.  102.]\n   [  99.   87.   97.]]\n\n  [[  95.   94.  102.]\n   [  97.   96.  104.]\n   [  97.   96.  104.]\n   ..., \n   [ 103.   91.  101.]\n   [ 103.   91.  101.]\n   [  98.   86.   96.]]\n\n  [[  95.   94.  102.]\n   [  97.   96.  104.]\n   [  97.   96.  104.]\n   ..., \n   [ 103.   91.  101.]\n   [ 103.   91.  101.]\n   [  98.   86.   96.]]\n\n  ..., \n  [[  86.   76.   87.]\n   [  70.   60.   71.]\n   [  70.   60.   71.]\n   ..., \n   [  19.    9.   20.]\n   [  19.    9.   20.]\n   [  19.    9.   20.]]\n\n  [[  86.   76.   87.]\n   [  70.   60.   71.]\n   [  70.   60.   71.]\n   ..., \n   [  19.    9.   20.]\n   [  19.    9.   20.]\n   [  19.    9.   20.]]\n\n  [[  80.   70.   81.]\n   [  72.   62.   73.]\n   [  72.   62.   73.]\n   ..., \n   [  19.    9.   20.]\n   [  19.    9.   20.]\n   [  20.   10.   21.]]]\n\n\n [[[  85.   77.   75.]\n   [  88.   80.   78.]\n   [  87.   79.   77.]\n   ..., \n   [  78.   76.   64.]\n   [  78.   76.   64.]\n   [  80.   78.   66.]]\n\n  [[  88.   80.   78.]\n   [  88.   80.   78.]\n   [  86.   78.   76.]\n   ..., \n   [  79.   75.   64.]\n   [  79.   75.   64.]\n   [  79.   75.   64.]]\n\n  [[  84.   76.   74.]\n   [  91.   83.   81.]\n   [  91.   83.   81.]\n   ..., \n   [  85.   76.   67.]\n   [  84.   75.   66.]\n   [  84.   75.   66.]]\n\n  ..., \n  [[  58.   53.   50.]\n   [  57.   52.   49.]\n   [  57.   52.   49.]\n   ..., \n   [   9.    9.    9.]\n   [  10.   10.   10.]\n   [  13.   13.   13.]]\n\n  [[  61.   56.   53.]\n   [  58.   53.   50.]\n   [  57.   52.   49.]\n   ..., \n   [  10.   10.   10.]\n   [  10.   10.   10.]\n   [   9.    9.    9.]]\n\n  [[  57.   52.   48.]\n   [  56.   51.   47.]\n   [  58.   53.   49.]\n   ..., \n   [  18.   18.   18.]\n   [  15.   15.   15.]\n   [  12.   12.   12.]]]\n\n\n [[[ 134.  155.  174.]\n   [ 131.  152.  171.]\n   [ 129.  150.  169.]\n   ..., \n   [ 103.  122.  136.]\n   [ 103.  122.  136.]\n   [ 101.  120.  134.]]\n\n  [[ 130.  151.  170.]\n   [ 130.  151.  170.]\n   [ 130.  151.  170.]\n   ..., \n   [ 103.  122.  136.]\n   [ 103.  122.  136.]\n   [ 101.  120.  134.]]\n\n  [[ 127.  148.  167.]\n   [ 129.  150.  169.]\n   [ 130.  151.  170.]\n   ..., \n   [ 103.  122.  136.]\n   [ 103.  122.  136.]\n   [ 101.  120.  134.]]\n\n  ..., \n  [[ 152.  152.  126.]\n   [ 166.  164.  139.]\n   [ 171.  169.  144.]\n   ..., \n   [  17.   10.   18.]\n   [  17.   10.   18.]\n   [  17.   10.   18.]]\n\n  [[ 152.  152.  126.]\n   [ 164.  164.  138.]\n   [ 170.  168.  143.]\n   ..., \n   [  17.   10.   18.]\n   [  17.   10.   18.]\n   [  17.   10.   18.]]\n\n  [[ 151.  151.  125.]\n   [ 163.  163.  137.]\n   [ 169.  167.  142.]\n   ..., \n   [  17.   10.   18.]\n   [  17.   10.   18.]\n   [  17.   10.   18.]]]\n\n\n ..., \n [[[  15.   15.   15.]\n   [  15.   15.   15.]\n   [  38.   38.   38.]\n   ..., \n   [  18.   18.   18.]\n   [  18.   18.   18.]\n   [  18.   18.   18.]]\n\n  [[  15.   15.   15.]\n   [  15.   15.   15.]\n   [  38.   38.   38.]\n   ..., \n   [  18.   18.   18.]\n   [  18.   18.   18.]\n   [  18.   18.   18.]]\n\n  [[  15.   15.   15.]\n   [  15.   15.   15.]\n   [  35.   35.   35.]\n   ..., \n   [  18.   18.   18.]\n   [  18.   18.   18.]\n   [  18.   18.   18.]]\n\n  ..., \n  [[  79.   76.   95.]\n   [  79.   76.   95.]\n   [  79.   76.   95.]\n   ..., \n   [  61.   59.   72.]\n   [  59.   57.   70.]\n   [  59.   57.   70.]]\n\n  [[  77.   74.   93.]\n   [  77.   74.   93.]\n   [  77.   74.   93.]\n   ..., \n   [  60.   58.   71.]\n   [  59.   57.   70.]\n   [  59.   57.   70.]]\n\n  [[  77.   74.   93.]\n   [  77.   74.   93.]\n   [  77.   74.   93.]\n   ..., \n   [  60.   58.   71.]\n   [  59.   57.   70.]\n   [  59.   57.   70.]]]\n\n\n [[[ 154.  126.  122.]\n   [ 154.  126.  122.]\n   [ 158.  130.  126.]\n   ..., \n   [  94.   73.   72.]\n   [  99.   75.   75.]\n   [  99.   75.   75.]]\n\n  [[ 154.  126.  122.]\n   [ 154.  126.  122.]\n   [ 158.  130.  126.]\n   ..., \n   [  94.   73.   72.]\n   [  99.   75.   75.]\n   [  99.   75.   75.]]\n\n  [[ 158.  130.  126.]\n   [ 158.  130.  126.]\n   [ 159.  131.  127.]\n   ..., \n   [  85.   64.   63.]\n   [  85.   61.   61.]\n   [  85.   61.   61.]]\n\n  ..., \n  [[ 233.   87.  100.]\n   [ 233.   87.  100.]\n   [ 247.  101.  114.]\n   ..., \n   [ 199.   49.   61.]\n   [ 199.   49.   61.]\n   [ 199.   49.   61.]]\n\n  [[ 236.   90.  103.]\n   [ 236.   90.  103.]\n   [ 244.   98.  111.]\n   ..., \n   [ 198.   48.   60.]\n   [ 197.   46.   61.]\n   [ 197.   46.   61.]]\n\n  [[ 236.   90.  103.]\n   [ 236.   90.  103.]\n   [ 244.   98.  111.]\n   ..., \n   [ 198.   48.   60.]\n   [ 197.   46.   61.]\n   [ 197.   46.   61.]]]\n\n\n [[[  57.   59.   37.]\n   [  57.   59.   37.]\n   [  55.   57.   36.]\n   ..., \n   [  46.   24.    0.]\n   [  49.   33.    0.]\n   [  59.   47.    7.]]\n\n  [[  57.   59.   37.]\n   [  49.   51.   30.]\n   [  45.   47.   26.]\n   ..., \n   [  48.   26.    3.]\n   [  51.   35.    2.]\n   [  61.   49.    9.]]\n\n  [[  47.   49.   28.]\n   [  45.   47.   26.]\n   [  46.   48.   27.]\n   ..., \n   [  48.   26.    3.]\n   [  52.   35.    5.]\n   [  63.   51.   13.]]\n\n  ..., \n  [[  41.   36.   33.]\n   [  42.   37.   34.]\n   [  39.   34.   30.]\n   ..., \n   [  22.   17.   21.]\n   [  27.   22.   26.]\n   [  30.   25.   29.]]\n\n  [[  39.   33.   33.]\n   [  41.   36.   33.]\n   [  39.   34.   30.]\n   ..., \n   [  23.   18.   22.]\n   [  28.   23.   27.]\n   [  30.   25.   29.]]\n\n  [[  40.   34.   36.]\n   [  42.   36.   36.]\n   [  39.   34.   31.]\n   ..., \n   [  30.   25.   29.]\n   [  29.   24.   28.]\n   [  26.   21.   25.]]]]"
     ]
    }
   ],
   "source": [
    "ft_model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=no_of_epochs,\n",
    "                validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ft_model.save(path+'cats_dogs_ep_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ft_model = load_model(path+'cats_dogs_ep_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_evaluate(val_path):\n",
    "        val_batches = get_batches(val_path, batch_size=batch_size, shuffle=False, class_mode='categorical')\n",
    "        return ft_model.evaluate_generator(val_batches, val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = model_evaluate(valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_predict(image_path, batch_size = batch_size):\n",
    "    p_batches = get_batches(image_path, batch_size=batch_size, shuffle=False, class_mode=None)\n",
    "    pred = ft_model.predict_generator(p_batches,p_batches.nb_sample)\n",
    "    return p_batches, pred\n",
    "    \n",
    "#test_batches = get_batches(test_path, batch_size=batch_size, shuffle=False, class_mode=None)\n",
    "#val_batches = get_batches(valid_path, batch_size=batch_size, shuffle=False, class_mode=None)\n",
    "#t_batches = get_batches(train_path, batch_size=batch_size, shuffle=False, class_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_batches, val_predict = model_predict(valid_path,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_filenames = val_batches.filenames\n",
    "val_classes = val_batches.classes\n",
    "save_array(path + 'val_predict.dat', val_predict)\n",
    "save_array(path + 'val_filenames.dat', val_batches.filenames)\n",
    "save_array(path + 'val_classes.dat', val_batches.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches, test_predict = model_predict(test_path,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(path + 'test_predict.dat', test_predict)\n",
    "save_array(path + 'test_filenames.dat', test_batches.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zipping results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ep_dir = \"ep2\"\n",
    "%cd $DATA_HOME_DIR\n",
    "%mkdir $ep_dir\n",
    "%mv *.dat $ep_dir\n",
    "%zip -r $ep_dir.zip $ep_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_predict = load_array((path + 'val_predict.dat')\n",
    "val_filenames = load_array(path + 'val_filenames.dat')\n",
    "val_classes = load_array(path + 'val_classes.dat')\n",
    "# test_predict = load_array((path + 'test_predict.dat')\n",
    "# test_filenames =  load_array(path + 'test_filenames.dat') \n",
    "# no known classes for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Couple of cats examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "our_prediction = 1-val_predict[:,0]\n",
    "our_class_prediction = np.round(our_prediction)\n",
    "correct_cats = np.where((val_classes == 0) & (our_class_prediction == 0))[0]\n",
    "print \"correct cats %d\" % len(correct_cats)\n",
    "idx = permutation(correct_cats)[:n_view]\n",
    "plots_array(valid_path, np.array(val_filenames)[idx], our_class_prediction[correct_cats][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Couple of dogs examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_dogs = np.where((val_classes == 1) & (our_class_prediction == 1))[0]\n",
    "print \"correct dogs %d\" % len(correct_dogs)\n",
    "idx = permutation(correct_dogs)[:n_view]\n",
    "plots_array(valid_path, np.array(val_filenames[idx]), our_class_prediction[correct_dogs][idx])\n",
    "plots_array(valid_path, val_batches.filenames[-8:], our_class_prediction[-8:])\n",
    "val_batches.classes[-8:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most wrong cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "incorrect_cats = np.where((val_batches.classes == 0) & (our_class_prediction == 1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"total incorrect cats: \", len(incorrect_cats), \" which is %\", 100*len(incorrect_cats)/len(np.where(val_batches.classes == 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_most_incorrect = np.argsort(our_prediction[incorrect_cats])[::-1][:n_view]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plots_array(valid_path, np.array(val_batches.filenames)[idx_most_incorrect], our_prediction[incorrect_cats][idx_most_incorrect])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borderline recognizing cats but seen as dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_most_incorrect = np.argsort(our_prediction[incorrect_cats])[:n_view]\n",
    "plots_array(valid_path, np.array(val_batches.filenames)[idx_most_incorrect], our_prediction[incorrect_cats][idx_most_incorrect])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most borderline false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_most_incorrect = np.argsort(our_prediction[incorrect_cats])[::-1[:n_view]\n",
    "plots_array(valid_path, val_batches.filenames[idx_most_incorrect], our_prediction[idx_most_incorrect])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most wrong dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t_predict = np.round(1-t_predict[:,0])\n",
    "val_predict = np.round(1-val_predict[:,0])\n",
    "val_batches.classes[:1]\n",
    "#t_predict[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print val_batches.classes[:8]\n",
    "print v_predict[:]\n",
    "print test_batches.classes[:8]\n",
    "print test_predict[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_batches.classes[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_predict = ft_model.predict_generator(test_batches,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_batches.filenames[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.round(test_predict[:10,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plots_array(test_path,test_batches.filenames[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(val_classes, our_class_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, val_batches.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the format Kaggle requires for new submissions:\n",
    "```\n",
    "imageId,isDog\n",
    "1242, .3984\n",
    "3947, .1000\n",
    "4539, .9082\n",
    "2345, .0000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create single 'unknown' class for test set\n",
    "%cd $DATA_HOME_DIR/test\n",
    "%mv *.jpg unknown/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "%mkdir valid\n",
    "%mkdir results\n",
    "%mkdir -p sample/train\n",
    "%mkdir -p sample/test\n",
    "%mkdir -p sample/valid\n",
    "%mkdir -p sample/results\n",
    "%mkdir -p test/unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_DIR/train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = glob('*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(2000): os.rename(shuf[i], DATA_HOME_DIR+'/valid/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = glob(\"*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuf = np.random.permutation(g)\n",
    "for i in range(200): copyfile(shuf[i], DATA_HOME_DIR+'/sample/train/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_DIR/valid\n",
    "g = glob('*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(50): copyfile(shuf[i], DATA_HOME_DIR+'/sample/valid/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Divide cat/dog images into separate directories\n",
    "\n",
    "%cd $DATA_HOME_DIR/sample/train\n",
    "%mkdir cats\n",
    "%mkdir dogs\n",
    "%mv cat.*.jpg cats/\n",
    "%mv dog.*.jpg dogs/\n",
    "\n",
    "%cd $DATA_HOME_DIR/sample/valid\n",
    "%mkdir cats\n",
    "%mkdir dogs\n",
    "%mv cat.*.jpg cats/\n",
    "%mv dog.*.jpg dogs/\n",
    "\n",
    "%cd $DATA_HOME_DIR/valid\n",
    "%mkdir cats\n",
    "%mkdir dogs\n",
    "%mv cat.*.jpg cats/\n",
    "%mv dog.*.jpg dogs/\n",
    "\n",
    "%cd $DATA_HOME_DIR/train\n",
    "%mkdir cats\n",
    "%mkdir dogs\n",
    "%mv cat.*.jpg cats/\n",
    "%mv dog.*.jpg dogs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create single 'unknown' class for test set\n",
    "%cd $DATA_HOME_DIR/test\n",
    "%mv *.jpg unknown/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
