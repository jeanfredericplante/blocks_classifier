{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "import numpy as np\n",
    "import pdb # python debugger\n",
    "import theano \n",
    "# from theano.tensor import nnet\n",
    "from theano.tensor import *\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.np_utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create the inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.gutenberg.org/cache/epub/19657/pg19657.txt\n",
      "('corpus length:', 1087507)\n",
      "The Project Gutenberg EBook of Notre-Dame de Paris, by Victor Hugo\n",
      "\n",
      "This eBook is for the use of \n"
     ]
    }
   ],
   "source": [
    "notre_dame = \"http://www.gutenberg.org/cache/epub/19657/pg19657.txt\"\n",
    "lesmiserable1 = \"http://www.gutenberg.org/cache/epub/17489/pg17489.txt\"\n",
    "\n",
    "notredame_path = get_file('nd.txt', origin=str(notre_dame))\n",
    "import codecs\n",
    "\n",
    "text = codecs.open(notredame_path, \"r\", \"utf-8\").read()\n",
    "print('corpus length:', len(text))\n",
    "print(text[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total chars:', 115)\n",
      "ï\n",
      "ñ\n",
      "ô\n",
      "ù\n",
      "û\n",
      "﻿\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)\n",
    "for i in chars[-6:]: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'The Project Gutenberg EBook of Notre-Dame de Pari'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "idx = [char_indices[c] for i,c in enumerate(text)]\n",
    "idx[1:10]\n",
    "\"\".join(indices_char[i] for i in idx[1:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nb sequences:', 135938)\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 42\n",
    "hidden_layers = 256\n",
    "character_set = sequence_length = 8\n",
    "vocab_size = len(chars)+1\n",
    "char_in = []\n",
    "char_out = []\n",
    "for i in range(0, len(text) - 1 - sequence_length, sequence_length):\n",
    "    char_in.append([char_indices[c] for c in text[i: i + sequence_length]])\n",
    "    char_out.append([char_indices[c] for c in text[i+1: i + 1 + character_set]])\n",
    "char_in = np.array(char_in)\n",
    "char_out = np.array(char_out)\n",
    "print('nb sequences:', len(char_in))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((135938, 8, 115), (135938, 8, 115))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'oh_x_rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0f68daa36501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0min_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moh_xs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moh_x_rnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'oh_x_rnn' is not defined"
     ]
    }
   ],
   "source": [
    "oh_ys = [to_categorical(o, vocab_size) for o in np.array(char_out)]\n",
    "oh_xs = [to_categorical(i, vocab_size) for i in np.array(char_in)]\n",
    "out_rnn = np.array(oh_ys)\n",
    "in_rnn = np.array(oh_xs)\n",
    "print((in_rnn.shape,out_rnn.shape))\n",
    "print(oh_x_rnn[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "- setup basic functions, relu, sigmoid, dist(sqrt) and their derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1/1+e(-x). Delta s - s2\n",
    "def sigmoid(x): return (1/1+np.exp(x))\n",
    "def sigmoid_d(x):\n",
    "    s = sigmoid(x)\n",
    "    return s - pow(s,2)\n",
    "\n",
    "def relu(x): return np.maximum(x,0)\n",
    "def relu_d(x): return (x>0)*1\n",
    "relu(np.array([3.,-3.])), relu_d(np.array([3.,-3.]))\n",
    "\n",
    "def tanh(x): return np.tanh(x)\n",
    "def tanh_d(x): return (1-pow(np.tanh(x),2))\n",
    "\n",
    "def dist(a,b): return pow(a-b,2)\n",
    "def dist_d(a,b): return 2*(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps= 1e-7\n",
    "def x_entropy(pred,actual):\n",
    "    return -np.sum(actual * np.log(np.clip(pred, eps, 1-eps))) # doesn't behave as theano that doesnt' have clipping\n",
    "# log prob represents the number of bits needed to encode it. or the amount of time. Log the amount of it, log prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_preds = np.array([0.2,0.6,2.1])\n",
    "test_actuals = np.array([0.,1.,0.])\n",
    "th_xentr=nnet.categorical_crossentropy(test_preds, test_actuals).eval()\n",
    "print(\"theano %f, self %f\" % (th_xentr,x_entropy(test_preds,test_actuals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def x_entropy_d(pred,actual): # d xentr / dpred\n",
    "    return - actual / pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_actuals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3b206d542602>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdvector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_actuals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_inp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_entropy_d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_actuals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_actuals' is not defined"
     ]
    }
   ],
   "source": [
    "test_inp = dvector()\n",
    "test_out = nnet.categorical_crossentropy(test_inp, test_actuals)\n",
    "test_grad = theano.function([test_inp], grad(test_out, test_inp))\n",
    "print(x_entropy_d(test_preds,test_actuals))\n",
    "print(test_grad(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scan(fn, start, seq):\n",
    "    res = []\n",
    "    prev = start\n",
    "    for s in seq:\n",
    "        app = fn(prev,s)\n",
    "        res.append(app)\n",
    "        prev = app\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.2040816326530612, 1.4914618908788004, 1.9131399331911023, 2.5617468842376345, 3.6140274328955453, 5.4253397137496471, 8.7504853053566389, 15.286506620990432, 29.077257058962019, 60.341340936657183, 136.46015312310797, 335.18813009740728, 890.27463087067235, 2544.6418024876352, 7790.7198035335778, 25440.085072762704, 88262.519640197133, 324230.66398439766, 1257221.9419803175, 5131519.130531908, 21992225.845136747, 98740606.835307851, 463476318.79838383, 2270088093.0737166, 11582082108.518963, 61455945882.937347, 338634803845.75684, 1935056021976.7534, 11452372374965.459, 70116565561014.023, 443594598447232.62, 2896944316390091.5, 19510033151198576.0, 1.3537574023280645e+17, 9.6696957309147469e+17, 7.1042662512843039e+18, 5.3644459448473313e+19, 4.1601825694734411e+20, 3.3111657185604943e+21, 2.7029924233146895e+22, 2.2616875378755566e+23, 1.9385893181790485e+24, 1.7012110343203894e+25, 1.5276180716346356e+26, 1.4029145555828288e+27, 1.3170218276900026e+28, 1.2632658347230637e+29, 1.2374848993205522e+30, 1.2374848993205522e+31]\n"
     ]
    }
   ],
   "source": [
    "print(scan(lambda x,y: x*y+1, 0, np.linspace(0,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 0.1318760946791574)\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights\n",
    "from numpy.random import random, permutation, randn, normal\n",
    "hidden_size = 100\n",
    "n_input = n_output = vocab_size\n",
    "scale=np.sqrt(2./n_input)\n",
    "print(n_input,scale)\n",
    "Wxh = normal(scale=scale, size=(hidden_size, n_input)) # Wxh * x\n",
    "Why = normal(scale=scale, size=(n_output, hidden_size))\n",
    "Whh = np.eye(hidden_size, dtype=np.float32)\n",
    "bh = np.zeros((hidden_size,1))\n",
    "by = np.zeros((n_output, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = {}\n",
    "xs[-1] = char_in[1]\n",
    "def one_hot(idx, n_categories):\n",
    "    oh = np.zeros((n_categories,1))\n",
    "    oh[idx] = 1\n",
    "    return oh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AsTensorError",
     "evalue": "('Cannot convert [[Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 ..., Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0]\\n [Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 ..., Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0]\\n [Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 ..., Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0]\\n ..., \\n [Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 ..., Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0]\\n [Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 ..., Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0]\\n [Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 ..., Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0]] to TensorType', <type 'numpy.ndarray'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAsTensorError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-37429b7ca2d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWxh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mcce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# categorical cross entropy for character t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jfp/anaconda3/envs/fast1/lib/python2.7/site-packages/theano/tensor/var.pyc\u001b[0m in \u001b[0;36m__rdiv__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rdiv__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rmod__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jfp/anaconda3/envs/fast1/lib/python2.7/site-packages/theano/tensor/basic.pyc\u001b[0m in \u001b[0;36mdiv_proxy\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   3409\u001b[0m     \u001b[0;34m\"\"\"Proxy for either true_div or int_div, depending on types of x, y.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3410\u001b[0m     f = scal.int_or_true_div(\n\u001b[0;32m-> 3411\u001b[0;31m         \u001b[0mas_tensor_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdiscrete_dtypes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3412\u001b[0m         as_tensor_variable(y).dtype in discrete_dtypes)\n\u001b[1;32m   3413\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mscal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_div\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jfp/anaconda3/envs/fast1/lib/python2.7/site-packages/theano/tensor/basic.pyc\u001b[0m in \u001b[0;36mas_tensor_variable\u001b[0;34m(x, name, ndim)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mstr_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAsTensorError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot convert %s to TensorType\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;31m# this has a different name, because _as_tensor_variable is the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAsTensorError\u001b[0m: ('Cannot convert [[Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 ..., Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0]\\n [Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 ..., Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0]\\n [Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 ..., Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0]\\n ..., \\n [Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 ..., Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0]\\n [Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 ..., Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0]\\n [Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 ..., Elemwise{exp,no_inplace}.0\\n  Elemwise{exp,no_inplace}.0 Elemwise{exp,no_inplace}.0]] to TensorType', <type 'numpy.ndarray'>)"
     ]
    }
   ],
   "source": [
    "train_id = 1\n",
    "xs,hs,ys,ps = {},{},{},{}\n",
    "hprev = np.zeros((hidden_size,1))\n",
    "inputs = char_in[train_id]\n",
    "targets = char_out[train_id]\n",
    "t = 1\n",
    "xs[t] = one_hot(inputs[t], vocab_size)\n",
    "target = char_out\n",
    "np.dot(Wxh,xs[t])\n",
    "\n",
    "hs[t] = tanh(np.dot(Wxh,xs[t]) + np.dot(Whh,hprev)+ bh)\n",
    "ys[t] = np.dot(Why, hs[t]) + by\n",
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "cce = -np.log(ps[t][targets[t],0]) # categorical cross entropy for character t\n",
    "loss += cce\n",
    "ps[t][targets[t],0] # targets[t] is the target vocab index, 0 getst the value in the array\n",
    "# ps[t][targets[t]] = array([ 0.01005999]), ps[t][targets[t],0] = 0.01005999\n",
    "# loss cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'oh_x_rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-0b648a085b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moh_x_rnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'oh_x_rnn' is not defined"
     ]
    }
   ],
   "source": [
    "# heavily based on https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs, targets are list of integers (not oh encoding)\n",
    "    hprev previous hidden state for stateful\n",
    "    returns loss, gradients and last hidden state\n",
    "    \"\"\"\n",
    "    xs,hs,ys,ps = {},{},{},{}\n",
    "    hs[-1] = hprev #np.copy\n",
    "    loss = 0\n",
    "    \n",
    "    ### forward pass ###\n",
    "    ### inputs is ??\n",
    "    ### xs[t] is ???\n",
    "    for t in xrange(len(inputs)):\n",
    "        hs[t] = tanh(np.dot(Wxh,xs[t]) + np.dot(Whh,hprev)+ bh)\n",
    "        ys[t] = np.dot(Why, hs[t]) + by\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "        cce = -np.log(ps[t][targets[t],0]) # categorical cross entropy for character t\n",
    "        loss += cce\n",
    "        ps[t][targets[t],0] # targets[t] is the target vocab index, 0 getst the value in the array\n",
    "        # ps[t][targets[t]] = array([ 0.01005999]), ps[t][targets[t],0] = 0.01005999\n",
    "        # loss cross entropy\n",
    "    \n",
    "    ### backward pass ###\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hprev)\n",
    "    for t in reversed(xrange(len(inputs))): # starting from the last charachter\n",
    "        dscores  = np.copy(ps[t]) # dloss / dscores\n",
    "        dscores[targets[t]] -= 1\n",
    "        dWhy = \n",
    "        dby\n",
    "        dh\n",
    "        dhraw\n",
    "        dbh\n",
    "        dWxh\n",
    "        dWhh\n",
    "        dhnext\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
