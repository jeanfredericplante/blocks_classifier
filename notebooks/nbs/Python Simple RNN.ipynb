{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "import numpy as np\n",
    "import pdb # python debugger\n",
    "import theano \n",
    "# from theano.tensor import nnet\n",
    "from theano.tensor import *\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.np_utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('corpus length:', 1087507)\n",
      "The Project Gutenberg EBook of Notre-Dame de Paris, by Victor Hugo\r\n",
      "\r\n",
      "This eBook is for the use of \n"
     ]
    }
   ],
   "source": [
    "notre_dame = \"http://www.gutenberg.org/cache/epub/19657/pg19657.txt\"\n",
    "lesmiserable1 = \"http://www.gutenberg.org/cache/epub/17489/pg17489.txt\"\n",
    "\n",
    "notredame_path = get_file('nd.txt', origin=str(notre_dame))\n",
    "import codecs\n",
    "\n",
    "text = codecs.open(notredame_path, \"r\", \"utf-8\").read()\n",
    "print('corpus length:', len(text))\n",
    "print(text[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total chars:', 115)\n",
      "ï\n",
      "ñ\n",
      "ô\n",
      "ù\n",
      "û\n",
      "﻿\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)\n",
    "for i in chars[-6:]: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'The Project Gutenberg EBook of Notre-Dame de Pari'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "idx = [char_indices[c] for i,c in enumerate(text)]\n",
    "idx[1:10]\n",
    "\"\".join(indices_char[i] for i in idx[1:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nb sequences:', 135938)\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 42\n",
    "hidden_layers = 256\n",
    "character_set = sequence_length = 8\n",
    "vocab_size = len(chars)+1\n",
    "char_in = []\n",
    "char_out = []\n",
    "for i in range(0, len(text) - 1 - sequence_length, sequence_length):\n",
    "    char_in.append([char_indices[c] for c in text[i: i + sequence_length]])\n",
    "    char_out.append([char_indices[c] for c in text[i+1: i + 1 + character_set]])\n",
    "char_in = np.array(char_in)\n",
    "char_out = np.array(char_out)\n",
    "print('nb sequences:', len(char_in))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((135938, 8, 115), (135938, 8, 115))\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "oh_ys = [to_categorical(o, vocab_size) for o in np.array(char_out)]\n",
    "oh_xs = [to_categorical(i, vocab_size) for i in np.array(char_in)]\n",
    "out_rnn = np.array(oh_ys)\n",
    "in_rnn = np.array(oh_xs)\n",
    "print((in_rnn.shape,out_rnn.shape))\n",
    "print(oh_x_rnn[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "- setup basic functions, relu, sigmoid, dist(sqrt) and their derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1/1+e(-x). Delta s - s2\n",
    "def sigmoid(x): return (1/1+np.exp(x))\n",
    "def sigmoid_d(x):\n",
    "    s = sigmoid(x)\n",
    "    return s - pow(s,2)\n",
    "\n",
    "def relu(x): return np.maximum(x,0)\n",
    "def relu_d(x): return (x>0)*1\n",
    "relu(np.array([3.,-3.])), relu_d(np.array([3.,-3.]))\n",
    "\n",
    "def tanh(x): return np.tanh(x)\n",
    "def tanh_d(x): return (1-pow(np.tanh(x),2))\n",
    "\n",
    "def dist(a,b): return pow(a-b,2)\n",
    "def dist_d(a,b): return 2*(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps= 1e-7\n",
    "def x_entropy(pred,actual):\n",
    "    return -np.sum(actual * np.log(np.clip(pred, eps, 1-eps))) # doesn't behave as theano that doesnt' have clipping\n",
    "# log prob represents the number of bits needed to encode it. or the amount of time. Log the amount of it, log prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theano 0.510826, self 0.510826\n"
     ]
    }
   ],
   "source": [
    "test_preds = np.array([0.2,0.6,2.1])\n",
    "test_actuals = np.array([0.,1.,0.])\n",
    "th_xentr=nnet.categorical_crossentropy(test_preds, test_actuals).eval()\n",
    "print(\"theano %f, self %f\" % (th_xentr,x_entropy(test_preds,test_actuals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def x_entropy_d(pred,actual): # d xentr / dpred\n",
    "    return - actual / pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.         -1.66666667 -0.        ]\n",
      "[-0.         -1.66666667 -0.        ]\n"
     ]
    }
   ],
   "source": [
    "test_inp = dvector()\n",
    "test_out = nnet.categorical_crossentropy(test_inp, test_actuals)\n",
    "test_grad = theano.function([test_inp], grad(test_out, test_inp))\n",
    "print(x_entropy_d(test_preds,test_actuals))\n",
    "print(test_grad(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scan(fn, start, seq):\n",
    "    res = []\n",
    "    prev = start\n",
    "    for s in seq:\n",
    "        app = fn(prev,s)\n",
    "        res.append(app)\n",
    "        prev = app\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.2040816326530612, 1.4914618908788004, 1.9131399331911023, 2.5617468842376345, 3.6140274328955453, 5.4253397137496471, 8.7504853053566389, 15.286506620990432, 29.077257058962019, 60.341340936657183, 136.46015312310797, 335.18813009740728, 890.27463087067235, 2544.6418024876352, 7790.7198035335778, 25440.085072762704, 88262.519640197133, 324230.66398439766, 1257221.9419803175, 5131519.130531908, 21992225.845136747, 98740606.835307851, 463476318.79838383, 2270088093.0737166, 11582082108.518963, 61455945882.937347, 338634803845.75684, 1935056021976.7534, 11452372374965.459, 70116565561014.023, 443594598447232.62, 2896944316390091.5, 19510033151198576.0, 1.3537574023280645e+17, 9.6696957309147469e+17, 7.1042662512843039e+18, 5.3644459448473313e+19, 4.1601825694734411e+20, 3.3111657185604943e+21, 2.7029924233146895e+22, 2.2616875378755566e+23, 1.9385893181790485e+24, 1.7012110343203894e+25, 1.5276180716346356e+26, 1.4029145555828288e+27, 1.3170218276900026e+28, 1.2632658347230637e+29, 1.2374848993205522e+30, 1.2374848993205522e+31]\n"
     ]
    }
   ],
   "source": [
    "print(scan(lambda x,y: x*y+1, 0, np.linspace(0,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 0.1318760946791574)\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights\n",
    "from numpy.random import random, permutation, randn, normal\n",
    "hidden_size = 100\n",
    "n_input = n_output = vocab_size\n",
    "scale=np.sqrt(2./n_input)\n",
    "print(n_input,scale)\n",
    "Wxh = normal(scale=scale, size=(hidden_size, n_input)) # Wxh * x\n",
    "Why = normal(scale=scale, size=(n_output, hidden_size))\n",
    "Whh = np.eye(hidden_size, dtype=np.float32)\n",
    "bh = np.zeros((hidden_size,1))\n",
    "by = np.zeros((n_output, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = {}\n",
    "xs[-1] = char_in[1]\n",
    "def one_hot(idx, n_categories):\n",
    "    oh = np.zeros((n_categories,1))\n",
    "    oh[idx] = 1\n",
    "    return oh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010059990120497152"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_id = 1\n",
    "xs,hs,ys,ps = {},{},{},{}\n",
    "hprev = np.zeros((hidden_size,1))\n",
    "inputs = char_in[train_id]\n",
    "targets = char_out[train_id]\n",
    "t = 1\n",
    "xs[t] = one_hot(inputs[t], vocab_size)\n",
    "target = char_out\n",
    "np.dot(Wxh,xs[t])\n",
    "\n",
    "hs[t] = tanh(np.dot(Wxh,xs[t]) + np.dot(Whh,hprev)+ bh)\n",
    "ys[t] = np.dot(Why, hs[t]) + by\n",
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "cce = -np.log(ps[t][targets[t],0]) # categorical cross entropy for character t\n",
    "loss += cce\n",
    "ps[t][targets[t],0] # targets[t] is the target vocab index, 0 getst the value in the array\n",
    "# ps[t][targets[t]] = array([ 0.01005999]), ps[t][targets[t],0] = 0.01005999\n",
    "# loss cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'oh_x_rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-0b648a085b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moh_x_rnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'oh_x_rnn' is not defined"
     ]
    }
   ],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs, targets are list of integers (not oh encoding)\n",
    "    hprev previous hidden state for stateful\n",
    "    returns loss, gradients and last hidden state\n",
    "    \"\"\"\n",
    "    xs,hs,ys,ps = {},{},{},{}\n",
    "    hs[-1] = hprev #np.copy\n",
    "    loss = 0\n",
    "    \n",
    "    ### forward pass ###\n",
    "    for t in xrange(len(inputs)):\n",
    "        hs[t] = tanh(np.dot(Wxh,xs[t]) + np.dot(Whh,hprev)+ bh)\n",
    "        ys[t] = np.dot(Why, hs[t]) + by\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "        cce = -np.log(ps[t][targets[t],0]) # categorical cross entropy for character t\n",
    "        loss += cce\n",
    "        ps[t][targets[t],0] # targets[t] is the target vocab index, 0 getst the value in the array\n",
    "        # ps[t][targets[t]] = array([ 0.01005999]), ps[t][targets[t],0] = 0.01005999\n",
    "        # loss cross entropy\n",
    "    \n",
    "    ### backward pass ###\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hprev)\n",
    "    for t in reversed(xrange(len(inputs))): # starting from the last charachter\n",
    "        dscores  = np.copy(ps[t]) # dloss / dscores\n",
    "        dscores[targets[t]] -= 1\n",
    "        dWhy = \n",
    "        dby\n",
    "        dh\n",
    "        dhraw\n",
    "        dbh\n",
    "        dWxh\n",
    "        dWhh\n",
    "        dhnext\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
